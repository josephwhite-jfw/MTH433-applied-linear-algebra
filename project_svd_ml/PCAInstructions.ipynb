{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d675056-21b8-4583-a07a-387bf75ca820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896aff3e-1cdb-43b7-b42a-0e286c731231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pitch_data():\n",
    "    \"\"\"Splits the data from 433pitch.csv\n",
    "    into 2 dataframes. The first df has a random 70% sample of the data\n",
    "    from 433pitch.csv; the 'training' data.\n",
    "    The second df has the remaining 30%; the 'testing' data. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    two dfs\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('433pitch.csv', index_col=0)\n",
    "    df0 = df.loc[df['injury'] < 0.5] #slice out the uninjured samples\n",
    "    df1 = df.loc[df['injury'] > 0.5] #slice out the injured samples\n",
    "    \n",
    "    size0 = df0.shape[0] \n",
    "    size1 = df1.shape[0] \n",
    "    rand0 = np.random.permutation(size0) #randomly shuffle the inj samples\n",
    "    rand1 = np.random.permutation(size1) #randomly shuffle the uninj samples\n",
    "\n",
    "    per0 = int(0.7 * size0) \n",
    "    per1 = int(0.7 * size1)\n",
    "    \n",
    "    df_train = pd.concat([df0.iloc[rand0[:per0]],df1.iloc[rand1[:per1]]]) #stack the uninjured on top of the injured\n",
    "    df_test = df.drop(df_train.index) #df_test is 30% left over\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21990105-75a6-49ff-9511-a117f57cc111",
   "metadata": {},
   "source": [
    "Fix k=1,2,3,4. Do this\n",
    "\n",
    "1. Split the data randomly into df_train and df_test (normalize and standardize everything)\n",
    "2. Get V for the df_train data\n",
    "3. Set train_data = first k V-coordinates of df_train data and train_label = df_train['injury]\n",
    "4. Set test_data = first k V-coordinates of df_test and test_label = df_test['injury']\n",
    "5. Then run your comparison function on train_data, etc..\n",
    "6. Repeat 100 times and return the average of these 100 runs for k=1,2,3,4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2922a6c-5914-4bbb-ae47-5566110a3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test():\n",
    "    \"\"\"Make a function that does Steps 1-6 above. 2 loops allowed (k=1,2,3,4 and repeat 100 times)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    1x4 numpy array\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405b6a5-2e1d-49be-b57b-7c2f28d7e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run your test here and leave the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8eac8-2344-4d3d-bea6-cc1efc12abe4",
   "metadata": {},
   "source": [
    "Which of k=1,2,3,4 seems to do a better job at predicting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c9a38-b3bb-4003-87a2-c0a778758ae7",
   "metadata": {},
   "source": [
    "To wrap it up calculate the cumulative variance for all of the data and compare this to the results you got in final_test. Ruminate for a few seconds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
